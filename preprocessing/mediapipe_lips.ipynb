{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615e698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55211bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright 2023 Imperial College London (Pingchuan Ma)\n",
    "# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class FunctionalModule(torch.nn.Module):\n",
    "    def __init__(self, functional):\n",
    "        super().__init__()\n",
    "        self.functional = functional\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.functional(input)\n",
    "\n",
    "\n",
    "class VideoTransform:\n",
    "    def __init__(self, speed_rate):\n",
    "        self.video_pipeline = torch.nn.Sequential(\n",
    "            FunctionalModule(lambda x: x.unsqueeze(-1)),\n",
    "            FunctionalModule(lambda x: x if speed_rate == 1 else torch.index_select(x, dim=0, index=torch.linspace(0, x.shape[0]-1, int(x.shape[0] / speed_rate), dtype=torch.int64))),\n",
    "            FunctionalModule(lambda x: x.permute(3, 0, 1, 2)),\n",
    "            FunctionalModule(lambda x: x / 255.),\n",
    "            torchvision.transforms.CenterCrop(88),\n",
    "            torchvision.transforms.Normalize(0.421, 0.165),\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.video_pipeline(sample)\n",
    "\n",
    "\n",
    "class AudioTransform:\n",
    "    def __init__(self):\n",
    "        self.audio_pipeline = torch.nn.Sequential(\n",
    "            FunctionalModule(lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=0)),\n",
    "            FunctionalModule(lambda x: x.transpose(0, 1)),\n",
    "        )\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.audio_pipeline(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8cdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright 2023 Imperial College London (Pingchuan Ma)\n",
    "# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import transform as tf\n",
    "\n",
    "\n",
    "def linear_interpolate(landmarks, start_idx, stop_idx):\n",
    "    start_landmarks = landmarks[start_idx]\n",
    "    stop_landmarks = landmarks[stop_idx]\n",
    "    delta = stop_landmarks - start_landmarks\n",
    "    for idx in range(1, stop_idx-start_idx):\n",
    "        landmarks[start_idx+idx] = start_landmarks + idx/float(stop_idx-start_idx) * delta\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def warp_img(src, dst, img, std_size):\n",
    "    tform = tf.estimate_transform('similarity', src, dst)\n",
    "    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)\n",
    "    warped = (warped * 255).astype('uint8')\n",
    "    return warped, tform\n",
    "\n",
    "\n",
    "def apply_transform(transform, img, std_size):\n",
    "    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)\n",
    "    warped = (warped * 255).astype('uint8')\n",
    "    return warped\n",
    "\n",
    "\n",
    "def cut_patch(img, landmarks, height, width, threshold=5):\n",
    "    center_x, center_y = np.mean(landmarks, axis=0)\n",
    "    # Check for too much bias in height and width\n",
    "    if abs(center_y - img.shape[0] / 2) > height + threshold:\n",
    "        raise Exception('too much bias in height')\n",
    "    if abs(center_x - img.shape[1] / 2) > width + threshold:\n",
    "        raise Exception('too much bias in width')\n",
    "    # Calculate bounding box coordinates\n",
    "    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))\n",
    "    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))\n",
    "    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))\n",
    "    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))\n",
    "    # Cut the image\n",
    "    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])\n",
    "    return cutted_img\n",
    "\n",
    "\n",
    "class VideoProcess:\n",
    "    def __init__(self, mean_face_path=\"20words_mean_face.npy\", crop_width=96, crop_height=96,\n",
    "                 start_idx=3, stop_idx=4, window_margin=12, convert_gray=True):\n",
    "        self.reference = np.load(os.path.join(os.path.dirname(''), mean_face_path))\n",
    "        self.crop_width = crop_width\n",
    "        self.crop_height = crop_height\n",
    "        self.start_idx = start_idx\n",
    "        self.stop_idx = stop_idx\n",
    "        self.window_margin = window_margin\n",
    "        self.convert_gray = convert_gray\n",
    "\n",
    "    def __call__(self, video, landmarks):\n",
    "        # Pre-process landmarks: interpolate frames that are not detected\n",
    "        preprocessed_landmarks = self.interpolate_landmarks(landmarks)\n",
    "        # Exclude corner cases: no landmark in all frames\n",
    "        if not preprocessed_landmarks:\n",
    "            return\n",
    "        # Affine transformation and crop patch\n",
    "        sequence = self.crop_patch(video, preprocessed_landmarks)\n",
    "        assert sequence is not None, f\"cannot crop a patch from {filename}.\"\n",
    "        return sequence\n",
    "\n",
    "\n",
    "    def crop_patch(self, video, landmarks):\n",
    "        sequence = []\n",
    "        for frame_idx, frame in enumerate(video):\n",
    "            window_margin = min(self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx)\n",
    "            smoothed_landmarks = np.mean([landmarks[x] for x in range(frame_idx - window_margin, frame_idx + window_margin + 1)], axis=0)\n",
    "            smoothed_landmarks += landmarks[frame_idx].mean(axis=0) - smoothed_landmarks.mean(axis=0)\n",
    "            transformed_frame, transformed_landmarks = self.affine_transform(frame,smoothed_landmarks,self.reference,grayscale=self.convert_gray)\n",
    "            patch = cut_patch(transformed_frame, transformed_landmarks[self.start_idx:self.stop_idx], self.crop_height//2, self.crop_width//2,)\n",
    "            sequence.append(patch)\n",
    "        return np.array(sequence)\n",
    "\n",
    "\n",
    "    def interpolate_landmarks(self, landmarks):\n",
    "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
    "\n",
    "        if not valid_frames_idx:\n",
    "            return None\n",
    "\n",
    "        for idx in range(1, len(valid_frames_idx)):\n",
    "            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:\n",
    "                landmarks = linear_interpolate(landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx])\n",
    "\n",
    "        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]\n",
    "\n",
    "        # Handle corner case: keep frames at the beginning or at the end that failed to be detected\n",
    "        if valid_frames_idx:\n",
    "            landmarks[:valid_frames_idx[0]] = [landmarks[valid_frames_idx[0]]] * valid_frames_idx[0]\n",
    "            landmarks[valid_frames_idx[-1]:] = [landmarks[valid_frames_idx[-1]]] * (len(landmarks) - valid_frames_idx[-1])\n",
    "\n",
    "        assert all(lm is not None for lm in landmarks), \"not every frame has landmark\"\n",
    "\n",
    "        return landmarks\n",
    "\n",
    "\n",
    "    def affine_transform(self, frame, landmarks, reference, grayscale=False,\n",
    "                         target_size=(256, 256), reference_size=(256, 256), stable_points=(0, 1, 2, 3),\n",
    "                         interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, border_value=0):\n",
    "        if grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        stable_reference = self.get_stable_reference(reference, reference_size, target_size)\n",
    "        transform = self.estimate_affine_transform(landmarks, stable_points, stable_reference)\n",
    "        transformed_frame, transformed_landmarks = self.apply_affine_transform(frame, landmarks, transform, target_size, interpolation, border_mode, border_value)\n",
    "\n",
    "        return transformed_frame, transformed_landmarks\n",
    "\n",
    "\n",
    "    def get_stable_reference(self, reference, reference_size, target_size):\n",
    "        # -- right eye, left eye, nose tip, mouth center\n",
    "        stable_reference = np.vstack([\n",
    "            np.mean(reference[36:42], axis=0),\n",
    "            np.mean(reference[42:48], axis=0),\n",
    "            np.mean(reference[31:36], axis=0),\n",
    "            np.mean(reference[48:68], axis=0)\n",
    "        ])\n",
    "        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0\n",
    "        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0\n",
    "        return stable_reference\n",
    "\n",
    "\n",
    "    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):\n",
    "        return cv2.estimateAffinePartial2D(np.vstack([landmarks[x] for x in stable_points]), stable_reference, method=cv2.LMEDS)[0]\n",
    "\n",
    "\n",
    "    def apply_affine_transform(self, frame, landmarks, transform, target_size, interpolation, border_mode, border_value):\n",
    "        transformed_frame = cv2.warpAffine(frame, transform, dsize=(target_size[0], target_size[1]),\n",
    "                                           flags=interpolation, borderMode=border_mode, borderValue=border_value)\n",
    "        transformed_landmarks = np.matmul(landmarks, transform[:, :2].transpose()) + transform[:, 2].transpose()\n",
    "        return transformed_frame, transformed_landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd7cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright 2021 Imperial College London (Pingchuan Ma)\n",
    "# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "import warnings\n",
    "import torchvision\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LandmarksDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_face_detection = mp.solutions.face_detection\n",
    "        self.short_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
    "        self.full_range_detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
    "\n",
    "    def __call__(self, filename):\n",
    "        video_frames = torchvision.io.read_video(filename, pts_unit='sec')[0].numpy()\n",
    "        landmarks = self.detect(video_frames, self.full_range_detector)\n",
    "        if all(element is None for element in landmarks):\n",
    "            landmarks = self.detect(video_frames, self.short_range_detector)\n",
    "            assert any(l is not None for l in landmarks), \"Cannot detect any frames in the video\"\n",
    "        return landmarks\n",
    "\n",
    "    def detect(self, video_frames, detector):\n",
    "        landmarks = []\n",
    "        for frame in video_frames:\n",
    "            results = detector.process(frame)\n",
    "            if not results.detections:\n",
    "                landmarks.append(None)\n",
    "                continue\n",
    "            face_points = []\n",
    "            for idx, detected_faces in enumerate(results.detections):\n",
    "                max_id, max_size = 0, 0\n",
    "                bboxC = detected_faces.location_data.relative_bounding_box\n",
    "                ih, iw, ic = frame.shape\n",
    "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
    "                if bbox_size > max_size:\n",
    "                    max_id, max_size = idx, bbox_size\n",
    "                lmx = [\n",
    "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].x * iw),\n",
    "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(0).value].y * ih)],\n",
    "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].x * iw),\n",
    "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(1).value].y * ih)],\n",
    "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].x * iw),\n",
    "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(2).value].y * ih)],\n",
    "                    [int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].x * iw),\n",
    "                     int(detected_faces.location_data.relative_keypoints[self.mp_face_detection.FaceKeyPoint(3).value].y * ih)],\n",
    "                    ]\n",
    "                face_points.append(lmx)\n",
    "            landmarks.append(np.array(face_points[max_id]))\n",
    "        return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c42d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class AVSRDataLoader:\n",
    "    def __init__(self, modality, speed_rate=1, transform=True, detector=\"retinaface\", convert_gray=True):\n",
    "        self.modality = modality\n",
    "        self.transform = transform\n",
    "        if self.modality in [\"audio\", \"audiovisual\"]:\n",
    "            self.audio_transform = AudioTransform()\n",
    "        if self.modality in [\"video\", \"audiovisual\"]:\n",
    "            if detector == \"mediapipe\":\n",
    "                self.video_process = VideoProcess(convert_gray=convert_gray)\n",
    "            self.video_transform = VideoTransform(speed_rate=speed_rate)\n",
    "\n",
    "\n",
    "    def load_data(self, data_filename, landmarks=None, transform=True):\n",
    "        if self.modality == \"audio\":\n",
    "            audio, sample_rate = self.load_audio(data_filename)\n",
    "            audio = self.audio_process(audio, sample_rate)\n",
    "            return self.audio_transform(audio) if self.transform else audio\n",
    "        if self.modality == \"video\":\n",
    "            video = self.load_video(data_filename)\n",
    "            video = self.video_process(video, landmarks)\n",
    "            video = torch.tensor(video)\n",
    "            return self.video_transform(video) if self.transform else video\n",
    "        if self.modality == \"audiovisual\":\n",
    "            rate_ratio = 640\n",
    "            audio, sample_rate = self.load_audio(data_filename)\n",
    "            audio = self.audio_process(audio, sample_rate)\n",
    "            video = self.load_video(data_filename)\n",
    "            video = self.video_process(video, landmarks)\n",
    "            video = torch.tensor(video)\n",
    "            min_t = min(len(video), audio.size(1) // rate_ratio)\n",
    "            audio = audio[:, :min_t*rate_ratio]\n",
    "            video = video[:min_t]\n",
    "            if self.transform:\n",
    "                audio = self.audio_transform(audio)\n",
    "                video = self.video_transform(video)\n",
    "            return video, audio\n",
    "\n",
    "\n",
    "    def load_audio(self, data_filename):\n",
    "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
    "        return waveform, sample_rate\n",
    "\n",
    "\n",
    "    def load_video(self, data_filename):\n",
    "        return torchvision.io.read_video(data_filename, pts_unit='sec')[0].numpy()\n",
    "\n",
    "\n",
    "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, target_sample_rate)\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041ddb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "def save2vid(filename, vid, frames_per_second):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    torchvision.io.write_video(filename, vid, frames_per_second)\n",
    "\n",
    "def preprocess_video(src_filename, dst_filename):\n",
    "    landmarks = landmarks_detector(src_filename)\n",
    "    data = dataloader.load_data(src_filename, landmarks)\n",
    "    fps = 25 # cv2.VideoCapture(src_filename).get(cv2.CAP_PROP_FPS)\n",
    "    save2vid(dst_filename, data, fps)\n",
    "    torchvision.io.write_video(dst_filename, data, fps)\n",
    "    return\n",
    "\n",
    "dataloader = AVSRDataLoader(modality=\"video\", speed_rate=1, transform=False, detector=\"mediapipe\", convert_gray=False)\n",
    "landmarks_detector = LandmarksDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce91cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_video(src_filename=\"./taseen.mp4\", dst_filename=\"./output/roi.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoavsr",
   "language": "python",
   "name": "autoavsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
